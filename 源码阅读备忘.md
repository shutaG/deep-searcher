
# 一些不错的主意：
1. 让llm根据查到的东西和问题，来判断是否可以生成更多的问题，这样可以在向量库中获取到更多的内容。
  - 只让模型回答是、否
  - 解析模型回答中的列表或者字典数据格式的方法




# 应该思考的地方：
1. 最后将所有的问题、子问题、库中检索到的内容都放到了一次提问中，感觉这样容易导致内容过长的问题。