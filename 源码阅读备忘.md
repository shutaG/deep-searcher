
# 一些不错的主意：

1. (query)让llm根据查到的东西和问题，来判断是否可以生成更多的问题，这样可以在向量库中获取到更多的内容。
  - 只让模型回答是、否
  - 解析模型回答中的列表或者字典数据格式的方法
2. (load_from_local_files)tqdm可以显示循环的进度条





# 应该思考的地方：
1. (query)最后将所有的问题、子问题、库中检索到的内容都放到了一次提问中，感觉这样容易导致内容过长的问题。
2. (load_from_local_files)为什么给向量库中存储的时候要增加wider_text放到metadata中

# 项目逻辑
1. 用户输入问题
2. 如果问题可以拆分，使用大模型将问题转化为子问题
3. 从向量库中查找与问题匹配的内容
4. 交给大模型判断是否与问题有关联，如果有关联，则加入知识列表
5. 判断收集到的所有内容，交给大模型判断是否能够回答，如果不能回答再次生成一些问题，并继续执行步骤2
6. 当收集到了足够多的内容或者超过设置的迭代次数后，交给大模型进行回答